{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Контекст \n",
    "\n",
    "Использовать модель transformer-BERT для классификации тональности IMDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import DistilBertTokenizerFast\n",
    "from transformers import DistilBertForSequenceClassification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train - val - test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.backends.cudnn.deterministic = True\n",
    "RANDOM_SEED = 42\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>33553</th>\n",
       "      <td>This is a clever and entertaining film about t...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9427</th>\n",
       "      <td>The Woman In Black is fantastic in all aspects...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>...about the importance of being young, having...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12447</th>\n",
       "      <td>Recap: It's business as usual at Louche's casi...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39489</th>\n",
       "      <td>There have been some harsh criticisms of Coman...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text sentiment\n",
       "33553  This is a clever and entertaining film about t...       pos\n",
       "9427   The Woman In Black is fantastic in all aspects...       pos\n",
       "199    ...about the importance of being young, having...       pos\n",
       "12447  Recap: It's business as usual at Louche's casi...       pos\n",
       "39489  There have been some harsh criticisms of Coman...       pos"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')\n",
    "data = pd.concat([train, test], axis = 0, ignore_index = True).sample(frac = 1, random_state = RANDOM_SEED)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>33553</th>\n",
       "      <td>This is a clever and entertaining film about t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9427</th>\n",
       "      <td>The Woman In Black is fantastic in all aspects...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>...about the importance of being young, having...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12447</th>\n",
       "      <td>Recap: It's business as usual at Louche's casi...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39489</th>\n",
       "      <td>There have been some harsh criticisms of Coman...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  sentiment\n",
       "33553  This is a clever and entertaining film about t...          1\n",
       "9427   The Woman In Black is fantastic in all aspects...          1\n",
       "199    ...about the importance of being young, having...          1\n",
       "12447  Recap: It's business as usual at Louche's casi...          1\n",
       "39489  There have been some harsh criticisms of Coman...          1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['sentiment'] = data['sentiment'].map(lambda x: np.where(x == 'neg', 0, 1))\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sentiment\n",
       "1    25000\n",
       "0    25000\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts = data.iloc[:35000]['text'].values\n",
    "train_labels = data.iloc[:35000]['sentiment'].values\n",
    "\n",
    "valid_texts = data.iloc[35000:40000]['text'].values\n",
    "valid_labels = data.iloc[35000:40000]['sentiment'].values\n",
    "\n",
    "test_texts = data.iloc[40000:]['text'].values\n",
    "test_labels = data.iloc[40000:]['sentiment'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Токенизация"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "train_encodings = tokenizer(\n",
    "    list(train_texts),\n",
    "    truncation = True,\n",
    "    padding = True\n",
    "    )\n",
    "\n",
    "valid_encodings = tokenizer(\n",
    "    list(valid_texts),\n",
    "    truncation = True,\n",
    "    padding = True\n",
    "    )\n",
    "\n",
    "test_encodings = tokenizer(\n",
    "    list(test_texts),\n",
    "    truncation = True,\n",
    "    padding = True\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset - Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IMDbDataset(Dataset):\n",
    "\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = IMDbDataset(train_encodings, train_labels)\n",
    "valid_dataset = IMDbDataset(valid_encodings, valid_labels)\n",
    "test_dataset = IMDbDataset(test_encodings, test_labels)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True) \n",
    "valid_loader = DataLoader(valid_dataset, batch_size=16, shuffle=False) \n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([  101,  2023,  2003,  1037, 12266,  1998, 14036,  2143,  2055,  1996,\n",
       "          2067,  9954, 17773,  2008,  2253,  2006,  2000,  5454,  1037,  6332,\n",
       "          2000,  5206,  9806,  2006,  1996,  3892,  2265,  1012,  2174,  1010,\n",
       "          1996,  3185,  1005,  1055,  7209,  9459,  1010,  2008,  2585, 26593,\n",
       "          1005,  1055,  6020, 21699, 11725,  2020, 17092,  2138,  1997,  2010,\n",
       "          6801,  3267,  1010,  1998,  1996,  2265, 15911,  2011,  1996,  4372,\n",
       "          2705, 20793,  3672,  1997,  2019, 14092,  6108, 18798,  2080,  1999,\n",
       "          1996,  2877,  4038,  2565,  2006,  2547,  2003,  3432,  4895, 16344,\n",
       "          5657,  1010,  2004,  3087,  2040,  2038,  3427,  2119, 18798,  2080,\n",
       "          1998, 26593,  4685,  2064,  2156,  2005,  3209,  1012,  1996,  2524,\n",
       "          1998,  3722,  3606,  2003,  3599,  2004,  2028,  6788,  4654,  8586,\n",
       "          3090,  1999,  1996,  2143,  1000,  6108, 18798,  2080,  2003,  1996,\n",
       "          4569, 15580,  2102,  2158,  1999,  2637,  1012,  1000,  1998,  2585,\n",
       "         26593,  2003,  2025,  1012,  2320,  2017,  5382,  2023,  1010,  2059,\n",
       "          2172,  1997,  1996, 24532, 22045,  1999,  1996,  2143,  2468, 22537,\n",
       "          1012,  4821,  1010,  1996,  2878,  2143,  8310,  2000,  2210,  2062,\n",
       "          2084, 14768, 16575,  2011, 26593,  4599,  1010,  2040,  2074,  3685,\n",
       "          5138,  2008,  1996,  2488,  5021,  2001,  4217,  2011,  2111,  3005,\n",
       "          2449,  2009,  2003,  2000,  2113,  2122,  2785,  1997,  2477,  1012,\n",
       "          1996,  2345,  6947,  2003,  2008,  2750, 26593,  1005,  1055,  2220,\n",
       "          2599,  1010,  4298,  2138,  1997,  1996,  1044, 18863,  2008,  6003,\n",
       "          2013,  1996,  2645,  2005,  3892,  2265,  8338,  1010, 18798,  2080,\n",
       "          1005,  1055,  2265,  2038, 10862,  4928,  2000,  2022,  1996,  2062,\n",
       "          2759,  1010,  2947,  1010,  1999,  2026,  2568,  2012,  2560,  1010,\n",
       "         25416, 20807, 26593,  1005,  1055,  4447,  2000,  2022, 15571,  2135,\n",
       "         20114,  1997,  2010, 27167, 12839,  1012,   102,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0]),\n",
       " 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " 'labels': tensor(1)}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loader.dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DistilBertForSequenceClassification(\n",
       "  (distilbert): DistilBertModel(\n",
       "    (embeddings): Embeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (transformer): Transformer(\n",
       "      (layer): ModuleList(\n",
       "        (0-5): 6 x TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = DistilBertForSequenceClassification.from_pretrained(\n",
    "    'distilbert-base-uncased'\n",
    ")\n",
    "\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(\n",
    "        model,\n",
    "        data_loader\n",
    "):\n",
    "    with torch.no_grad():\n",
    "\n",
    "        correct_predict = 0\n",
    "        full_samples = 0\n",
    "\n",
    "        for _, batch in enumerate(data_loader):\n",
    "\n",
    "            inputs = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            outputs = model(inputs, attention_mask = attention_mask)\n",
    "            logits = outputs['logits']\n",
    "\n",
    "            predicted_labels = torch.argmax(logits, 1)\n",
    "            full_samples += labels.size(0)\n",
    "            correct_predict += (predicted_labels == labels).sum()\n",
    "    \n",
    "    return correct_predict.float() / full_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Тренировочный цикл"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 3\n",
    "optim = torch.optim.Adam(model.parameters(), lr = 0.00005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001/0003 | Batch 0000/2188 | Loss: 0.6453\n",
      "Epoch: 0001/0003 | Batch 0250/2188 | Loss: 0.1840\n",
      "Epoch: 0001/0003 | Batch 0500/2188 | Loss: 0.3302\n",
      "Epoch: 0001/0003 | Batch 0750/2188 | Loss: 0.1972\n",
      "Epoch: 0001/0003 | Batch 1000/2188 | Loss: 0.4093\n",
      "Epoch: 0001/0003 | Batch 1250/2188 | Loss: 0.2997\n",
      "Epoch: 0001/0003 | Batch 1500/2188 | Loss: 0.3943\n",
      "Epoch: 0001/0003 | Batch 1750/2188 | Loss: 0.3038\n",
      "Epoch: 0001/0003 | Batch 2000/2188 | Loss: 0.2379\n",
      "Training accuracy: 0.96%\n",
      "Valid accuracy: 0.93%\n",
      "Time elapsed: 13.44 min\n",
      "Epoch: 0002/0003 | Batch 0000/2188 | Loss: 0.1864\n",
      "Epoch: 0002/0003 | Batch 0250/2188 | Loss: 0.0147\n",
      "Epoch: 0002/0003 | Batch 0500/2188 | Loss: 0.0549\n",
      "Epoch: 0002/0003 | Batch 0750/2188 | Loss: 0.0370\n",
      "Epoch: 0002/0003 | Batch 1000/2188 | Loss: 0.0755\n",
      "Epoch: 0002/0003 | Batch 1250/2188 | Loss: 0.1114\n",
      "Epoch: 0002/0003 | Batch 1500/2188 | Loss: 0.2735\n",
      "Epoch: 0002/0003 | Batch 1750/2188 | Loss: 0.0207\n",
      "Epoch: 0002/0003 | Batch 2000/2188 | Loss: 0.1159\n",
      "Training accuracy: 0.99%\n",
      "Valid accuracy: 0.93%\n",
      "Time elapsed: 27.02 min\n",
      "Epoch: 0003/0003 | Batch 0000/2188 | Loss: 0.0154\n",
      "Epoch: 0003/0003 | Batch 0250/2188 | Loss: 0.4123\n",
      "Epoch: 0003/0003 | Batch 0500/2188 | Loss: 0.3632\n",
      "Epoch: 0003/0003 | Batch 0750/2188 | Loss: 0.0104\n",
      "Epoch: 0003/0003 | Batch 1000/2188 | Loss: 0.0027\n",
      "Epoch: 0003/0003 | Batch 1250/2188 | Loss: 0.0333\n",
      "Epoch: 0003/0003 | Batch 1500/2188 | Loss: 0.0102\n",
      "Epoch: 0003/0003 | Batch 1750/2188 | Loss: 0.0022\n",
      "Epoch: 0003/0003 | Batch 2000/2188 | Loss: 0.0135\n",
      "Training accuracy: 0.98%\n",
      "Valid accuracy: 0.92%\n",
      "Time elapsed: 40.84 min\n",
      "Total Training Time: 40.84 min\n",
      "Test accuracy: 0.91%\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    " \n",
    "    model.train()\n",
    "    \n",
    "    for batch_idx, batch in enumerate(train_loader):\n",
    "    \n",
    "        ### Prepare data\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        \n",
    "        ### Forward\n",
    "        outputs = model(\n",
    "            input_ids, \n",
    "            attention_mask=attention_mask,\n",
    "            labels=labels\n",
    "            )\n",
    "        loss, logits = outputs['loss'], outputs['logits']\n",
    "        \n",
    "        ### Backward\n",
    "        optim.zero_grad()\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "    \n",
    "        ### verbose\n",
    "        if not batch_idx % 250:\n",
    "            print(\n",
    "                f'Epoch: {epoch+1:04d}/{NUM_EPOCHS:04d}'\n",
    "                f' | Batch '\n",
    "                f'{batch_idx:04d}/'\n",
    "                f'{len(train_loader):04d} | '\n",
    "                f'Loss: {loss:.4f}')\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.set_grad_enabled(False):\n",
    "        print(\n",
    "            f'Training accuracy: '\n",
    "            f'{compute_accuracy(model, train_loader):.2f}%'f'\\nValid accuracy: '\n",
    "            f'{compute_accuracy(model, valid_loader):.2f}%'\n",
    "            )\n",
    "    print(f'Time elapsed: {(time.time() - start_time)/60:.2f} min')\n",
    "\n",
    "print(f'Total Training Time: {(time.time() - start_time)/60:.2f} min')\n",
    "print(f'Test accuracy: {compute_accuracy(model, test_loader):.2f}%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "newenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{"metadata":{"kernelspec":{"display_name":"newenv","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Контекст\n\n- бинарная классификация текста с одни предложением (анализ тональности)\n- тонкая настройка BERT (DistilBertForSequenceClassification)\n- IMDB датасет\n- 25000 train\n- 12500 val\n- 12500 test","metadata":{}},{"cell_type":"code","source":"# база\nimport pandas as pd\nimport warnings\nimport numpy as np\nimport json\nimport time\n\n# визуализация\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# nltk\nfrom nltk.lm import Vocabulary\n\n# pytorch\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\n\n# sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score,\\\n                            precision_recall_fscore_support\n\n# transformers\nfrom transformers import DistilBertForSequenceClassification,\\\n                         DistilBertTokenizerFast,\\\n                         TrainingArguments,\\\n                         Trainer\n\n# datasets\nfrom datasets import load_dataset\n\n# константы\nRANDOM_STATE = 42\nwarnings.filterwarnings(\"ignore\")\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint('CUDA ?: ', torch.cuda.is_available())","metadata":{},"execution_count":11,"outputs":[{"name":"stdout","output_type":"stream","text":"CUDA ?:  True\n"}]},{"cell_type":"markdown","source":"# Загрузка модели - токенизатора","metadata":{}},{"cell_type":"code","source":"model = DistilBertForSequenceClassification.from_pretrained(\n    'distilbert-base-uncased',\n    id2label = {0: 'NEG', 1: 'POS'},\n    label2id = {'NEG': 0, 'POS': 1})\n\ntokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Чтение","metadata":{}},{"cell_type":"code","source":"train = pd.read_csv('train.csv')\ntrain.head()","metadata":{},"execution_count":28,"outputs":[{"execution_count":28,"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>text</th>\n","      <th>sentiment</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Now, I won't deny that when I purchased this o...</td>\n","      <td>neg</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>The saddest thing about this \"tribute\" is that...</td>\n","      <td>neg</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Last night I decided to watch the prequel or s...</td>\n","      <td>neg</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>I have to admit that i liked the first half of...</td>\n","      <td>neg</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>I was not impressed about this film especially...</td>\n","      <td>neg</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                                text sentiment\n","0  Now, I won't deny that when I purchased this o...       neg\n","1  The saddest thing about this \"tribute\" is that...       neg\n","2  Last night I decided to watch the prequel or s...       neg\n","3  I have to admit that i liked the first half of...       neg\n","4  I was not impressed about this film especially...       neg"]},"metadata":{}}]},{"cell_type":"code","source":"test = pd.read_csv('test.csv')\ntest.head()","metadata":{},"execution_count":29,"outputs":[{"execution_count":29,"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>text</th>\n","      <th>sentiment</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>My daughter liked it but I was aghast, that a ...</td>\n","      <td>neg</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>I... No words. No words can describe this. I w...</td>\n","      <td>neg</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>this film is basically a poor take on the old ...</td>\n","      <td>neg</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>This is a terrible movie, and I'm not even sur...</td>\n","      <td>neg</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>First of all this movie is a piece of reality ...</td>\n","      <td>pos</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                                text sentiment\n","0  My daughter liked it but I was aghast, that a ...       neg\n","1  I... No words. No words can describe this. I w...       neg\n","2  this film is basically a poor take on the old ...       neg\n","3  This is a terrible movie, and I'm not even sur...       neg\n","4  First of all this movie is a piece of reality ...       pos"]},"metadata":{}}]},{"cell_type":"code","source":"val, test = train_test_split(\n    test,\n    test_size = 0.5,\n    random_state = RANDOM_STATE,\n    stratify = test['sentiment']\n)","metadata":{},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"val['sentiment'].value_counts()","metadata":{},"execution_count":null,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":["sentiment\n","neg    6250\n","pos    6250\n","Name: count, dtype: int64"]},"metadata":{}}]},{"cell_type":"code","source":"test['sentiment'].value_counts()","metadata":{},"execution_count":null,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":["sentiment\n","pos    6250\n","neg    6250\n","Name: count, dtype: int64"]},"metadata":{}}]},{"cell_type":"markdown","source":"# Маскируем 'neg' --> 0, 'pos' --> 1","metadata":{}},{"cell_type":"code","source":"train['sentiment'] = train['sentiment'].map(lambda x: np.where(x == 'neg', 0, 1))\nval['sentiment'] = val['sentiment'].map(lambda x: np.where(x == 'neg', 0, 1))\ntest['sentiment'] = test['sentiment'].map(lambda x: np.where(x == 'neg', 0, 1))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train['sentiment'].value_counts()","metadata":{},"execution_count":null,"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":["sentiment\n","0    12500\n","1    12500\n","Name: count, dtype: int64"]},"metadata":{}}]},{"cell_type":"markdown","source":"# Токенизируем train","metadata":{}},{"cell_type":"code","source":"tokenize_data_train = tokenizer(\n                    train['text'].to_list(),\n                    padding = True,\n                    truncation = True,\n                    return_attention_mask=True\n                    )\n\ntokenize_data_val = tokenizer(\n                    val['text'].to_list(),\n                    padding = True,\n                    truncation = True,\n                    return_attention_mask=True\n                    )\n\ntrain['input_ids'], train['attention_mask'] = tokenize_data_train['input_ids'], tokenize_data_train['attention_mask']\nval['input_ids'], val['attention_mask'] = tokenize_data_val['input_ids'], tokenize_data_val['attention_mask']","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.head()","metadata":{},"execution_count":null,"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>text</th>\n","      <th>sentiment</th>\n","      <th>input_ids</th>\n","      <th>attention_mask</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Now, I won't deny that when I purchased this o...</td>\n","      <td>0</td>\n","      <td>[101, 2085, 1010, 1045, 2180, 1005, 1056, 9772...</td>\n","      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>The saddest thing about this \"tribute\" is that...</td>\n","      <td>0</td>\n","      <td>[101, 1996, 6517, 6155, 2102, 2518, 2055, 2023...</td>\n","      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Last night I decided to watch the prequel or s...</td>\n","      <td>0</td>\n","      <td>[101, 2197, 2305, 1045, 2787, 2000, 3422, 1996...</td>\n","      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>I have to admit that i liked the first half of...</td>\n","      <td>0</td>\n","      <td>[101, 1045, 2031, 2000, 6449, 2008, 1045, 4669...</td>\n","      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>I was not impressed about this film especially...</td>\n","      <td>0</td>\n","      <td>[101, 1045, 2001, 2025, 7622, 2055, 2023, 2143...</td>\n","      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                                text  sentiment  \\\n","0  Now, I won't deny that when I purchased this o...          0   \n","1  The saddest thing about this \"tribute\" is that...          0   \n","2  Last night I decided to watch the prequel or s...          0   \n","3  I have to admit that i liked the first half of...          0   \n","4  I was not impressed about this film especially...          0   \n","\n","                                           input_ids  \\\n","0  [101, 2085, 1010, 1045, 2180, 1005, 1056, 9772...   \n","1  [101, 1996, 6517, 6155, 2102, 2518, 2055, 2023...   \n","2  [101, 2197, 2305, 1045, 2787, 2000, 3422, 1996...   \n","3  [101, 1045, 2031, 2000, 6449, 2008, 1045, 4669...   \n","4  [101, 1045, 2001, 2025, 7622, 2055, 2023, 2143...   \n","\n","                                      attention_mask  \n","0  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n","1  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n","2  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n","3  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n","4  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  "]},"metadata":{}}]},{"cell_type":"markdown","source":"# Trainer args","metadata":{}},{"cell_type":"code","source":"training_args = TrainingArguments(\n\n    output_dir = 'training/model_points', # сохранение контрольных точек модели\n    do_train = True, # мониторинг производительности\n    do_eval = True,\n    num_train_epochs = 3,\n    per_device_train_batch_size = 32,\n    per_gpu_eval_batch_size = 64,\n    warmup_steps = 100, # оптмизация скорости обучения\n    weight_decay = 0.01, # регуляризация весов модели\n    logging_strategy = 'steps', # аналог verbose с сохранением логов (также есть 'epoch')\n    logging_dir = 'training/logs',\n    save_steps = 200,\n    logging_steps = 100,\n    evaluation_strategy = 'steps',\n    fp16 = True, # Указывает на использование смешанной точности и использует как 16-, так и 32-битные типы с плавающей запятой, чтобы \n                 # обучение проходило быстрее и занимало меньше памяти\n    load_best_model_at_end = True\n    \n)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Функция подсчета метрик","metadata":{}},{"cell_type":"code","source":"def compute_metrics(pred):\n    labels = pred.label_ids\n    preds = pred.predictions.argmax(-1)\n    Precision, Recall, f1, _ = precision_recall_fscore_support(labels, preds, average='macro')\n    acc = accuracy_score(labels, preds)\n    return {\n            'Accuracy': acc,\n            'F1': f1,\n            'Precision': Precision,\n            'Recall': Recall\n            }","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Dataset","metadata":{}},{"cell_type":"code","source":"class ImdbDataset(Dataset):\n\n    def __init__(self, df):\n        self.data = df\n\n    def __len__(self):\n        return len(self.data)\n    \n    def __getitem__(self, index):\n        row = self.data.iloc[index]\n        return {\n            'input_ids': row['input_ids'],\n            'attention_mask': row['attention_mask'],\n            'labels': row['sentiment']\n        }","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Обучение","metadata":{}},{"cell_type":"code","source":"trainer = Trainer(\n\n    model = model,\n    args = training_args,\n    train_dataset = ImdbDataset(train),\n    eval_dataset = ImdbDataset(val),\n    compute_metrics = compute_metrics\n    \n)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"results = trainer.train()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Проверка модели","metadata":{}},{"cell_type":"code","source":"q=[trainer.evaluate(eval_dataset=ImdbDataset(data)) for data in [train, val]]\npd.DataFrame(q, index=[\"train\",\"val\"]).iloc[:,:5]","metadata":{},"execution_count":null,"outputs":[{"name":"stderr","output_type":"stream","text":"Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n\nUsing deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"4f579ddd8dbe4125a638d18d7a789042","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/391 [00:00<?, ?it/s]"]},"metadata":{}},{"name":"stderr","output_type":"stream","text":"Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n\nUsing deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n\nUsing deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"721b71afaa8e48998bfa53e62246bc4b","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/196 [00:00<?, ?it/s]"]},"metadata":{}},{"name":"stderr","output_type":"stream","text":"Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n"},{"execution_count":29,"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>eval_loss</th>\n","      <th>eval_Accuracy</th>\n","      <th>eval_F1</th>\n","      <th>eval_Precision</th>\n","      <th>eval_Recall</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>train</th>\n","      <td>0.140489</td>\n","      <td>0.94988</td>\n","      <td>0.949877</td>\n","      <td>0.949987</td>\n","      <td>0.94988</td>\n","    </tr>\n","    <tr>\n","      <th>val</th>\n","      <td>0.198395</td>\n","      <td>0.92160</td>\n","      <td>0.921570</td>\n","      <td>0.922254</td>\n","      <td>0.92160</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["       eval_loss  eval_Accuracy   eval_F1  eval_Precision  eval_Recall\n","train   0.140489        0.94988  0.949877        0.949987      0.94988\n","val     0.198395        0.92160  0.921570        0.922254      0.92160"]},"metadata":{}}]},{"cell_type":"code","source":"def get_prediction(text):\n    inputs = tokenizer(text, padding=True,truncation=True, max_length=250, return_tensors=\"pt\").to(device)\n    outputs = model(\n        inputs[\"input_ids\"].to(device),\n        inputs[\"attention_mask\"].to(device)\n        )\n    probs = outputs[0].softmax(1)\n    return probs, probs.argmax()\n\n\n# text = 'I don`t like this movie'\n# get_prediction(text)[1].item()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Сохранение","metadata":{}},{"cell_type":"code","source":"model_save_path = \"MyBestIMDB_binary_Model\"\ntrainer.save_model(model_save_path)\ntokenizer.save_pretrained(model_save_path)","metadata":{},"execution_count":null,"outputs":[{"execution_count":44,"output_type":"execute_result","data":{"text/plain":["('MyBestIMDB_binary_Model\\\\tokenizer_config.json',\n"," 'MyBestIMDB_binary_Model\\\\special_tokens_map.json',\n"," 'MyBestIMDB_binary_Model\\\\vocab.txt',\n"," 'MyBestIMDB_binary_Model\\\\added_tokens.json',\n"," 'MyBestIMDB_binary_Model\\\\tokenizer.json')"]},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import pipeline,\\\n                         DistilBertForSequenceClassification,\\\n                         DistilBertTokenizerFast\n\n\nmodel = DistilBertForSequenceClassification.from_pretrained('MyBestIMDB_binary_Model')\ntokenizer = DistilBertTokenizerFast.from_pretrained('MyBestIMDB_binary_Model')\n\nnlp = pipeline(\n    'sentiment-analysis',\n    model = model,\n    tokenizer = tokenizer\n)\n\nresults = []\nfor text in test['text']:\n    text = text[:512]\n    results.append(nlp(text)[0]['label'])","metadata":{},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"test['predict'] = results\ntest.head()","metadata":{},"execution_count":32,"outputs":[{"execution_count":32,"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>text</th>\n","      <th>sentiment</th>\n","      <th>predict</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>17392</th>\n","      <td>This is one of the best Czech movies I have ev...</td>\n","      <td>pos</td>\n","      <td>POS</td>\n","    </tr>\n","    <tr>\n","      <th>11969</th>\n","      <td>I saw this movie because every review I read o...</td>\n","      <td>neg</td>\n","      <td>NEG</td>\n","    </tr>\n","    <tr>\n","      <th>8306</th>\n","      <td>I've just watched this with my three children ...</td>\n","      <td>pos</td>\n","      <td>POS</td>\n","    </tr>\n","    <tr>\n","      <th>17625</th>\n","      <td>This film did well at the box office, and the ...</td>\n","      <td>neg</td>\n","      <td>NEG</td>\n","    </tr>\n","    <tr>\n","      <th>22088</th>\n","      <td>Surface was one of the few truly unique shows ...</td>\n","      <td>pos</td>\n","      <td>POS</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                                    text sentiment predict\n","17392  This is one of the best Czech movies I have ev...       pos     POS\n","11969  I saw this movie because every review I read o...       neg     NEG\n","8306   I've just watched this with my three children ...       pos     POS\n","17625  This film did well at the box office, and the ...       neg     NEG\n","22088  Surface was one of the few truly unique shows ...       pos     POS"]},"metadata":{}}]},{"cell_type":"code","source":"test['predict'] = test['predict'].map(lambda x: x.lower())\ntest.head()","metadata":{},"execution_count":33,"outputs":[{"execution_count":33,"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>text</th>\n","      <th>sentiment</th>\n","      <th>predict</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>17392</th>\n","      <td>This is one of the best Czech movies I have ev...</td>\n","      <td>pos</td>\n","      <td>pos</td>\n","    </tr>\n","    <tr>\n","      <th>11969</th>\n","      <td>I saw this movie because every review I read o...</td>\n","      <td>neg</td>\n","      <td>neg</td>\n","    </tr>\n","    <tr>\n","      <th>8306</th>\n","      <td>I've just watched this with my three children ...</td>\n","      <td>pos</td>\n","      <td>pos</td>\n","    </tr>\n","    <tr>\n","      <th>17625</th>\n","      <td>This film did well at the box office, and the ...</td>\n","      <td>neg</td>\n","      <td>neg</td>\n","    </tr>\n","    <tr>\n","      <th>22088</th>\n","      <td>Surface was one of the few truly unique shows ...</td>\n","      <td>pos</td>\n","      <td>pos</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                                    text sentiment predict\n","17392  This is one of the best Czech movies I have ev...       pos     pos\n","11969  I saw this movie because every review I read o...       neg     neg\n","8306   I've just watched this with my three children ...       pos     pos\n","17625  This film did well at the box office, and the ...       neg     neg\n","22088  Surface was one of the few truly unique shows ...       pos     pos"]},"metadata":{}}]},{"cell_type":"code","source":"print('Точность на тестовом наборе: ', (test['sentiment'] == test['predict']).sum() / len(test))","metadata":{},"execution_count":38,"outputs":[{"name":"stdout","output_type":"stream","text":"Точность на тестовом наборе:  0.85664\n"}]}]}
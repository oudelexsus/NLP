{"metadata":{"kernelspec":{"display_name":"base","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Контекст\n\n- модель 'distilbert-base-uncased'\n- оптимизатор AdamW (реализация алгоритма Adam, но с исправлением спада\tвеса)\n- используем Pytorch а не Trainer\n","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport warnings\n\nfrom transformers import DistilBertForSequenceClassification,\\\n                         DistilBertTokenizerFast,\\\n                         AdamW\n\nimport torch\nfrom torch.utils.data import Dataset,\\\n                             DataLoader\n\n\nfrom sklearn.model_selection import train_test_split\n\n\nwarnings.filterwarnings(\"ignore\")\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint('CUDA ?: ', torch.cuda.is_available())","metadata":{},"execution_count":131,"outputs":[{"name":"stdout","output_type":"stream","text":"CUDA ?:  True\n"}]},{"cell_type":"markdown","source":"# Скачиваем DistilBert оболочку","metadata":{}},{"cell_type":"code","source":"model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased')\ntokenizer = DistilBertTokenizerFast.from_pretrained('bert-base-uncased')","metadata":{},"execution_count":132,"outputs":[{"name":"stderr","output_type":"stream","text":"Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n\nThe tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n\nThe tokenizer class you load from this checkpoint is 'BertTokenizer'. \n\nThe class this function is called from is 'DistilBertTokenizerFast'.\n"}]},{"cell_type":"markdown","source":"# Чтение файла","metadata":{}},{"cell_type":"code","source":"train = pd.read_csv('train.csv')\ntrain.head()","metadata":{},"execution_count":134,"outputs":[{"execution_count":134,"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>text</th>\n","      <th>sentiment</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Now, I won't deny that when I purchased this o...</td>\n","      <td>neg</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>The saddest thing about this \"tribute\" is that...</td>\n","      <td>neg</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Last night I decided to watch the prequel or s...</td>\n","      <td>neg</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>I have to admit that i liked the first half of...</td>\n","      <td>neg</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>I was not impressed about this film especially...</td>\n","      <td>neg</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                                text sentiment\n","0  Now, I won't deny that when I purchased this o...       neg\n","1  The saddest thing about this \"tribute\" is that...       neg\n","2  Last night I decided to watch the prequel or s...       neg\n","3  I have to admit that i liked the first half of...       neg\n","4  I was not impressed about this film especially...       neg"]},"metadata":{}}]},{"cell_type":"code","source":"test = pd.read_csv('test.csv')\ntest.head()","metadata":{},"execution_count":135,"outputs":[{"execution_count":135,"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>text</th>\n","      <th>sentiment</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>My daughter liked it but I was aghast, that a ...</td>\n","      <td>neg</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>I... No words. No words can describe this. I w...</td>\n","      <td>neg</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>this film is basically a poor take on the old ...</td>\n","      <td>neg</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>This is a terrible movie, and I'm not even sur...</td>\n","      <td>neg</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>First of all this movie is a piece of reality ...</td>\n","      <td>pos</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                                text sentiment\n","0  My daughter liked it but I was aghast, that a ...       neg\n","1  I... No words. No words can describe this. I w...       neg\n","2  this film is basically a poor take on the old ...       neg\n","3  This is a terrible movie, and I'm not even sur...       neg\n","4  First of all this movie is a piece of reality ...       pos"]},"metadata":{}}]},{"cell_type":"code","source":"val, test = train_test_split(\n    test,\n    test_size = 0.5,\n    random_state = 42,\n    stratify = test['sentiment']\n)\n\nval['sentiment'].value_counts()","metadata":{},"execution_count":136,"outputs":[{"execution_count":136,"output_type":"execute_result","data":{"text/plain":["sentiment\n","neg    6250\n","pos    6250\n","Name: count, dtype: int64"]},"metadata":{}}]},{"cell_type":"code","source":"# обработка меток\ntrain['sentiment'] = train['sentiment'].map(lambda x: np.where(x == 'neg', 0, 1))\nval['sentiment'] = val['sentiment'].map(lambda x: np.where(x == 'neg', 0, 1))\ntest['sentiment'] = test['sentiment'].map(lambda x: np.where(x == 'neg', 0, 1))","metadata":{},"execution_count":137,"outputs":[]},{"cell_type":"markdown","source":"# Dataset","metadata":{}},{"cell_type":"code","source":"class ImdbDataset(Dataset):\n\n    def __init__(self, df):\n\n        self.data = df # берем не весь, для упрощения обучения\n        self.tokenize_texts = tokenizer(\n            self.data['text'].to_list(),\n            return_tensors = 'pt',\n            padding = True,\n            truncation = True,\n            max_length = 512\n            )\n\n    def __len__(self):\n        return len(self.data)\n    \n    def __getitem__(self, index):\n        row = self.data.iloc[index]\n        input_ids = self.tokenize_texts['input_ids'][index]\n        attention_mask = self.tokenize_texts['attention_mask'][index]\n\n        return {\n            'input_ids': input_ids,\n            'attention_mask': attention_mask,\n            'labels': row['sentiment']\n        }","metadata":{},"execution_count":138,"outputs":[]},{"cell_type":"markdown","source":"# Dataloader","metadata":{}},{"cell_type":"code","source":"torch.manual_seed(42)\n\ntrain_dataloader = DataLoader(\n    dataset = ImdbDataset(train),\n    batch_size = 64,\n    shuffle = True,\n    drop_last = True\n)\n\nval_dataloader = DataLoader(\n    dataset = ImdbDataset(val),\n    batch_size = 64,\n    shuffle = False,\n    drop_last = True\n)","metadata":{},"execution_count":139,"outputs":[]},{"cell_type":"code","source":"optimizer = AdamW(model.parameters(), lr = 0.001)\n\n\nfor epoch in range(3):\n\n    model = model.to(device)\n    model.train()\n\n    loss_train_all = 0.0\n    batch_size = 0.0\n\n    for batch in train_dataloader:\n\n        optimizer.zero_grad()\n\n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n        labels = batch['labels'].to(device)\n\n        outputs = model(input_ids, attention_mask = attention_mask, labels = labels)\n        loss = outputs.loss\n        loss_train_all += loss.item()\n\n\n        loss.backward()\n        optimizer.step()\n\n    loss_train = loss_train_all / len(train_dataloader)\n\n    model.eval()\n\n    kol_vo_correct_predict = 0\n    all_kol_vo = 0\n\n    for batch in val_dataloader:\n\n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n        labels = batch['labels'].to(device)\n\n        outputs = model(input_ids, attention_mask = attention_mask, labels = labels)\n        predictions = outputs.logits.argmax(dim=-1)\n\n        kol_vo_correct_predict += (predictions == labels).sum().item()\n        all_kol_vo += labels.size(0)\n    \n    val_accuracy = kol_vo_correct_predict / all_kol_vo\n\n    print('Epoch {} || train_loss: {:.3f} || val accuracy: {:.3f}'.format(epoch+1,\n                                                                          loss_train,\n                                                                          val_accuracy))","metadata":{},"execution_count":null,"outputs":[]}]}
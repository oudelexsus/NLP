{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Контекст\n",
    "\n",
    "- корпус текста - 'Эмма', Джейн Остин\n",
    "- модель на основе GPT2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.decoders import ByteLevel as ByteLevelDecoder\n",
    "from tokenizers.normalizers import Sequence, Lowercase\n",
    "from tokenizers.pre_tokenizers import ByteLevel\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "\n",
    "from transformers import GPT2TokenizerFast,\\\n",
    "                         GPT2Config,\\\n",
    "                         GPT2LMHeadModel\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Создание, настройка, обучение и сохранение токенизатора"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# настройки токенизатора\n",
    "tokenizer = Tokenizer(BPE())\n",
    "tokenizer.normalizer = Sequence([Lowercase()])\n",
    "tokenizer.pre_tokenizer = ByteLevel() # байты в качестве входных данных\n",
    "tokenizer.decoder = ByteLevelDecoder()\n",
    "\n",
    "# обучение токенизатора\n",
    "trainer = BpeTrainer(\n",
    "    vocab_size = 50000,\n",
    "    inital_alphabet=ByteLevel.alphabet(),\n",
    "    special_tokens=[\"<s>\",\"<pad>\",\"</s>\",\"<unk>\",\"<mask>\"]\n",
    "    )\n",
    "\n",
    "tokenizer.train([\"austen-emma.txt\"], trainer)\n",
    "\n",
    "\n",
    "# сохранение токенизатора в папке tokenizer_gpt\n",
    "tokenizer.save('tokenizer_gpt/tokenizer.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Добавление спец токенов к токенизатору"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11751"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# скачивание токенизатора под GPT2 архитектуру\n",
    "tokenizer_gpt = GPT2TokenizerFast.from_pretrained('tokenizer_gpt')\n",
    "len(tokenizer_gpt.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Токен начала последовательности:  0\n",
      "Токен конца последовательности:  2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0, 95, 37, 11, 264, 157, 312, 1143, 56, 2]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# добавление специальных токенов\n",
    "tokenizer_gpt.add_special_tokens({\n",
    "    \"eos_token\": \"</s>\",\n",
    "    \"bos_token\": \"<s>\",\n",
    "    \"unk_token\": \"<unk>\",\n",
    "    \"pad_token\": \"<pad>\",\n",
    "    \"mask_token\": \"<mask>\"\n",
    "})\n",
    "\n",
    "print('Токен начала последовательности: ', tokenizer_gpt.bos_token_id)\n",
    "print('Токен конца последовательности: ', tokenizer_gpt.eos_token_id)\n",
    "tokenizer_gpt.encode('<s> hi, what is your name </s>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Конфигурация модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = GPT2Config(\n",
    "    vocab_size = tokenizer_gpt.vocab_size,\n",
    "    bos_token_id = tokenizer_gpt.bos_token_id,\n",
    "    eos_token_id = tokenizer_gpt.eos_token_id\n",
    ")\n",
    "\n",
    "model = GPT2LMHeadModel(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2Config {\n",
       "  \"activation_function\": \"gelu_new\",\n",
       "  \"attn_pdrop\": 0.1,\n",
       "  \"bos_token_id\": 0,\n",
       "  \"embd_pdrop\": 0.1,\n",
       "  \"eos_token_id\": 2,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"layer_norm_epsilon\": 1e-05,\n",
       "  \"model_type\": \"gpt2\",\n",
       "  \"n_embd\": 768,\n",
       "  \"n_head\": 12,\n",
       "  \"n_inner\": null,\n",
       "  \"n_layer\": 12,\n",
       "  \"n_positions\": 1024,\n",
       "  \"reorder_and_upcast_attn\": false,\n",
       "  \"resid_pdrop\": 0.1,\n",
       "  \"scale_attn_by_inverse_layer_idx\": false,\n",
       "  \"scale_attn_weights\": true,\n",
       "  \"summary_activation\": null,\n",
       "  \"summary_first_dropout\": 0.1,\n",
       "  \"summary_proj_to_labels\": true,\n",
       "  \"summary_type\": \"cls_index\",\n",
       "  \"summary_use_proj\": true,\n",
       "  \"transformers_version\": \"4.39.3\",\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 11750\n",
       "}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Подготовка корпуса к предварительному обучению"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('austen-emma.txt', 'r', encoding = 'utf-8') as file:\n",
    "    content = file.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# удаление всех символов \\n и оставляем только строки длиной больше 10 для обучения модели на длинных строках,\n",
    "# чтобы она могла генерировать более длинные выходные последовательности\n",
    "\n",
    "content_new = []\n",
    "for stroka in content:\n",
    "    stroka = stroka.replace('\\n', '')\n",
    "    if len(stroka) > 10:\n",
    "        content_new.append(stroka.strip())\n",
    "\n",
    "content_new = ' '.join(content_new) + tokenizer_gpt.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Encoding(num_tokens=195130, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# сделаем одну длинную последовательность идентификаторов токенов\n",
    "tokenized_content = tokenizer.encode(content_new)\n",
    "tokenized_content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Возьмем последовательности токенов длиной 100 (нарастающим эффектом)\n",
    "\n",
    "- 0:100, 1:101, 2:102 и т.д. (100 выборок длиной 100 для ограничения времени обучения модели)\n",
    "- также разобъем каждый образец текста на x и target (x = 'Идет бычок', target = 'бычок качается')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = []\n",
    "for i in range(0, 100):\n",
    "    examples.append(tokenized_content.ids[i:i+100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = []\n",
    "labels = []\n",
    "\n",
    "for example in examples:\n",
    "    train_data.append(example[:-1])\n",
    "    labels.append(example[1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 6270,  1684,   236,   390, 11379, 10870,    26,   190,   371,    11,\n",
       "          1426,    11,  1739,    11,    83,  1707,    11,   148,    59,  1561,\n",
       "           619,    83,   656,  1911,    11,   596,    74,  5526,   392,    84,\n",
       "            71,   829,  4100,    84,  3840,    23,    83,   141,  1845,  2559,\n",
       "          2572,    12,   329,  1137,    97,    71,   866,   148,   167,   337,\n",
       "            74,  1771,   254,  2718,   105,    13,   120,   116,    71,  4873,\n",
       "            84,    71,   515,  3746,    84,    59,   429,  3296,    11,  7087,\n",
       "           476,    23,    83,   141,    11,    97,  1824,    84,   105,  1327,\n",
       "           176,  1559,    11,   204,  2805,    84,   166,   737,   242,    59,\n",
       "           167,  1397,  2182,    13,    56,   105,   939,   141,  4723]),\n",
       " tensor([ 1684,   236,   390, 11379, 10870,    26,   190,   371,    11,  1426,\n",
       "            11,  1739,    11,    83,  1707,    11,   148,    59,  1561,   619,\n",
       "            83,   656,  1911,    11,   596,    74,  5526,   392,    84,    71,\n",
       "           829,  4100,    84,  3840,    23,    83,   141,  1845,  2559,  2572,\n",
       "            12,   329,  1137,    97,    71,   866,   148,   167,   337,    74,\n",
       "          1771,   254,  2718,   105,    13,   120,   116,    71,  4873,    84,\n",
       "            71,   515,  3746,    84,    59,   429,  3296,    11,  7087,   476,\n",
       "            23,    83,   141,    11,    97,  1824,    84,   105,  1327,   176,\n",
       "          1559,    11,   204,  2805,    84,   166,   737,   242,    59,   167,\n",
       "          1397,  2182,    13,    56,   105,   939,   141,  4723,   419]))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class GPT_2Dataset(Dataset):\n",
    "    def __init__(self, x, target):\n",
    "        self.x = x\n",
    "        self.target = target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.x[idx]), torch.tensor(self.target[idx])\n",
    "    \n",
    "\n",
    "GPT_2Dataset(train_data, labels)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 6270,  1684,   236,   390, 11379, 10870,    26,   190,   371,    11,\n",
       "          1426,    11,  1739,    11,    83,  1707,    11,   148,    59,  1561,\n",
       "           619,    83,   656,  1911,    11,   596,    74,  5526,   392,    84,\n",
       "            71,   829,  4100,    84,  3840,    23,    83,   141,  1845,  2559,\n",
       "          2572,    12,   329,  1137,    97,    71,   866,   148,   167,   337,\n",
       "            74,  1771,   254,  2718,   105,    13,   120,   116,    71,  4873,\n",
       "            84,    71,   515,  3746,    84,    59,   429,  3296,    11,  7087,\n",
       "           476,    23,    83,   141,    11,    97,  1824,    84,   105,  1327,\n",
       "           176,  1559,    11,   204,  2805,    84,   166,   737,   242,    59,\n",
       "           167,  1397,  2182,    13,    56,   105,   939,   141,  4723]),\n",
       " tensor([ 1684,   236,   390, 11379, 10870,    26,   190,   371,    11,  1426,\n",
       "            11,  1739,    11,    83,  1707,    11,   148,    59,  1561,   619,\n",
       "            83,   656,  1911,    11,   596,    74,  5526,   392,    84,    71,\n",
       "           829,  4100,    84,  3840,    23,    83,   141,  1845,  2559,  2572,\n",
       "            12,   329,  1137,    97,    71,   866,   148,   167,   337,    74,\n",
       "          1771,   254,  2718,   105,    13,   120,   116,    71,  4873,    84,\n",
       "            71,   515,  3746,    84,    59,   429,  3296,    11,  7087,   476,\n",
       "            23,    83,   141,    11,    97,  1824,    84,   105,  1327,   176,\n",
       "          1559,    11,   204,  2805,    84,   166,   737,   242,    59,   167,\n",
       "          1397,  2182,    13,    56,   105,   939,   141,  4723,   419]))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    dataset = GPT_2Dataset(train_data, labels),\\\n",
    "    batch_size = 16,\n",
    "    shuffle = True,\n",
    "    drop_last = True\n",
    ")\n",
    "\n",
    "\n",
    "train_dataloader.dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Loss: 9.4835, Accuracy: 0.0000\n",
      "Epoch: 20, Loss: 2.1110, Accuracy: 0.6805\n",
      "Epoch: 40, Loss: 0.8857, Accuracy: 0.8621\n",
      "Epoch: 60, Loss: 0.3823, Accuracy: 0.9477\n",
      "Epoch: 80, Loss: 0.1917, Accuracy: 0.9721\n",
      "Epoch: 100, Loss: 0.1084, Accuracy: 0.9874\n",
      "Epoch: 120, Loss: 0.0699, Accuracy: 0.9921\n",
      "Epoch: 140, Loss: 0.0586, Accuracy: 0.9913\n",
      "Epoch: 160, Loss: 0.0396, Accuracy: 0.9958\n",
      "Epoch: 180, Loss: 0.0302, Accuracy: 0.9961\n",
      "Epoch: 200, Loss: 0.0265, Accuracy: 0.9960\n",
      "Epoch: 220, Loss: 0.0239, Accuracy: 0.9966\n",
      "Epoch: 240, Loss: 0.0208, Accuracy: 0.9965\n",
      "Epoch: 260, Loss: 0.0202, Accuracy: 0.9968\n",
      "Epoch: 280, Loss: 0.0179, Accuracy: 0.9968\n",
      "Epoch: 300, Loss: 0.0171, Accuracy: 0.9968\n"
     ]
    }
   ],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.00005)\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "\n",
    "for epoch in range(300):\n",
    "\n",
    "    total = 0.0\n",
    "    correct_predict = 0.0\n",
    "    total_loss = 0.0\n",
    "    batches_in_dataloader = 0\n",
    "\n",
    "    for batch in train_dataloader:\n",
    "\n",
    "        model.train()\n",
    "        \n",
    "        input_ids = batch[0].to(device)\n",
    "        targets = batch[1].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(input_ids)\n",
    "        logits = outputs[0]\n",
    "\n",
    "        batch_size, len_sequence, vocab_size = logits.size()\n",
    "\n",
    "        loss = loss_fn(\n",
    "            logits.view(-1, logits.size(2)),\n",
    "            targets.view(-1))\n",
    "        \n",
    "        \n",
    "        predict_tokens = torch.argmax(logits, dim = -1)\n",
    "\n",
    "        correct_predict += (predict_tokens == targets).sum().item()\n",
    "        total += targets.numel()\n",
    "        total_loss += loss.item()\n",
    "        batches_in_dataloader += 1\n",
    "    \n",
    "    accuracy = correct_predict / total\n",
    "    epoch_loss = total_loss / batches_in_dataloader\n",
    "    if (epoch + 1 == 1) or ((epoch+1) % 20 == 0):\n",
    "        print(\"Epoch: {}, Loss: {:.4f}, Accuracy: {:.4f}\".\\\n",
    "            format(epoch+1, epoch_loss, accuracy))\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Генерация текста"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"  her mother had died too long ago for her to have more than an indistinct remembrance of her caresses; and her place had been supplied by an excellent woman as governess, who had fallen little short of a mother in affection. sixteen years had miss taylor been in mr. woodhouse's family, less as a governess than a friend, very fond of both daughters, but particularly of emma.  between _them_ it was more the intimacy of sisters. even before ceased to hold the nominal office of governess had lived nearly twenty-one years in the world with very little to distress or vex her sister's marriage, with a comfortable home and happy disposition, indulgent father; clever, been mistress of his house from a most affectionate,  even a very early period. she was the and had ceased governess to unite some of the youngest of nominal a the two daughters consequence of. had had, her governess rich, nominal little of existence; 1816] her the closing been had home had nearly of before miss in consequence had nominal her had  of to had sisters seemed to jane austen 1816 her miss the a had even her nominal as to taylor had the best blessings of even and nominal by jane the pitch a or had d of and even daughters of mother, of in, in of nearly had indulgent but in nominal twenty had 1816  in governess lived hadbinet had in had and  governess died nominal governess her] emma woodhouse, even unite her even the her, hadma of but her  had henceforth a but governess ceased mother nominal emma had her in to in before, governessim sister the had daughters even nominal very her best hadored of had house had ide daughters governess an governess of indistinct in  a more clever had it office her as had gently had se, more lived her lived it, nearly- ago in even supplied  little her purchasing had mother she been even very in sisters her. but as than, than comfortable had to sha. jane, died the been, ceased an in and as her was her a  been ceased more in her of  taylor daughters mother ceased as indistinct a, and before little had gout her terri had little nominal his,- speculations a exper was had penet her her amount in adored the an a nominal miss her_ and governess'sabsolute had but indulgent the too a immediately, toassist had as nominalone her but even, clever a even some governess taylor her and in home clever of with governess little twenty in happy a home 1816 an very had jokes of little  very\""
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def generate(\n",
    "        start,\n",
    "        model,\n",
    "        max_length\n",
    "        ):\n",
    "\n",
    "    model = model.cpu()\n",
    "    input_token_ids = tokenizer_gpt.encode(start, return_tensors='pt')\n",
    "\n",
    "    \n",
    "    output = model.generate(\n",
    "                        input_token_ids,\n",
    "                        max_length = max_length,\n",
    "                        num_beams = 5,\n",
    "                        temperature = 0.7,\n",
    "                        no_repeat_ngram_size=2,\n",
    "                        num_return_sequences=1\n",
    "                        )\n",
    "    return tokenizer_gpt.decode(output[0])\n",
    "\n",
    "\n",
    "generate(' ', model, 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\"wetson was very good little to distress or vex her. she was the youngest of the two daughters of a most affectionate, indulgent father;'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate('\"wetson was very good', model, 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Сохранение - загрузка модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# сохранение\n",
    "model.save_pretrained('my_gpt2/')\n",
    "\n",
    "# загрузка\n",
    "# model_reloaded = TFGPT2LMHeadModel.from_pretrained(\"my_gpt-2/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('tokenizer_gpt/tokenizer_config.json',\n",
       " 'tokenizer_gpt/special_tokens_map.json',\n",
       " 'tokenizer_gpt/vocab.json',\n",
       " 'tokenizer_gpt/merges.txt',\n",
       " 'tokenizer_gpt/added_tokens.json',\n",
       " 'tokenizer_gpt/tokenizer.json')"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_gpt.save_pretrained('tokenizer_gpt/')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "newenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
